## The Models to Predict the Amount of Bike Sharing

### Regular Neural Network

Based on the exploratory data analysis, regular neural network is used to figure out the project.

Neural network is a model that optimize the parameters through learning process to recognize hidden relationships between different data. 

Because of the low correlation between given features, regular neural network probably is the most appropriate model to solve the project. The architecture of regular neural network is shown in Figure @fig:regular-neural-network .

![The Architecture of Regular Neural Network](images/Basic Model.png "Wide image"){#fig:regular-neural-network}

There are 4 steps to build and train neural network, including:

- Selecting features; 
- Data preprocessing; 
- Designing layers and parameters; 
- Determining training methods. 

We used different features, epochs, hidden layers, units and learning rates in this project. The architectures of our models are shown in Figure @fig:jingzi-model , @fig:anye-model and @fig:dana-model . The main parameters of our models are shown in Table @tbl:our-models

![The Architecture of Jingzi’s Model](images/Jingzi Model.png "Wide image"){#fig:jingzi-model}

![The Architecture of Anye’s Model](images/Anye Model.png "Wide image"){#fig:anye-model}

![The Architecture of Dana’s Model](images/Dana Model.png "Wide image"){#fig:dana-model}

| **Models** | **Features** | **Epochs** | **Hidden Layers** | **Units** | **Learning Rates** |
|:---------- |:---------:|:-------------:|:-------------:|:-------------:|:-------------:|
| Jingzi | 11 | <150 | 5 | 289 | Non-Constant |
| Anye |  12 | 300 | 4 | 131 | Non-Constant |
| Dana | 7 | 800 | 6 | 641 | Constant |
|*Epochs in Jingzi’s model will be stopped when loss reached the minimum. <!-- $colspan="6" --> | | | | | |
Table: The Overview of Our Models
{#tbl:our-models}

The evaluation of the models’ performances are based on the root mean squared error(RMSE) between the test data and predictions. The equation of RMSE is shown in equation @eq:rmse .

$$ RMSE = \sqrt{\frac{1}{n}\Sigma_{i=1}^{n}{(observed_i - predicted_i)^2}}$$ {#eq:rmse}

In equation @eq:rmse , “n” is the number of predictions. “observed_i” and “predicted_i” is the observed cnt and the predicted cnt at each group of features. Thus, RMSE represents the differences between observations and predictions.

### Sensitivity Analysis

To evaluate and optimize our models, we analyzed the sensitivity of different parameters.

Units of Layers, Layers, Normalization and Learning Rates are analyzed. To avoid the influence of randomization, the evaluation of the performances is according to the average RMSE of 3 separate training with same initial parameters.

The architectures and main training methods of sensitivity analysis are shown in Table @tbl:units-setting , Table @tbl:layers-setting , Table @tbl:normalization-setting and Table @tbl:rates-setting .

| **Layer 1** | **Layer 2** | **Layer 3** | **Layer 4** | **Layer 5** | **Learning Rate** | **Normalization** |
|:--------- |:----------:|:---------:|:---------:|:---------:|:------------------:|:------------------:|
| 32 | 16 | 128 | 64 | 1 | Same with Jingzi | At each layer |
| 32 | 32 | 128 | 64 | 1 | Same with Jingzi | At each layer |
| 32 | 64 | 128 | 64 | 1 | Same with Jingzi | At each layer |
| 32 | 128 | 128 | 64 | 1 | Same with Jingzi | At each layer |
Table: The Sensitivity of the Number of Units in Layer 2
{#tbl:units-setting}

| **Layer 1** | **Layer 2** | **Layer 3** | **Layer 4** | **Layer 5** | **Learning Rate** | **Normalization** |
|:--------- |:----------:|:---------:|:---------:|:---------:|:------------------:|:------------------:|
| 32 | 64 | 128 | 64 | 1 | Same with Jingzi | At each layer |
| - | 64 | 128 | 64 | 1 | Same with Jingzi | At each layer |
| - | - | 128 | 64 | 1 | Same with Jingzi | At each layer |
Table: The Sensitivity of the Number of Layers
{#tbl:layers-setting}

| **Layer 1** | **Layer 2** | **Layer 3** | **Layer 4** | **Layer 5** | **Learning Rate** | **Normalization** |
|:--------- |:----------:|:---------:|:---------:|:---------:|:------------------:|:------------------:|
| 32 | 64 | 128 | 64 | 1 | Same with Jingzi | At each layer |
| 32 | 64 | 128 | 64 | 1 | Same with Jingzi | - |
Table: The Sensitivity of Normalization
{#tbl:normalization-setting}

| **Layer 1** | **Layer 2** | **Layer 3** | **Layer 4** | **Layer 5** | **Learning Rate** | **Normalization** |
|:--------- |:----------:|:---------:|:---------:|:---------:|:------------------:|:------------------:|
| 32 | 64 | 128 | 64 | 1 | Same with Jingzi | At each layer |
| 32 | 64 | 128 | 64 | 1 | Same with Anye | At each layer |
| 32 | 64 | 128 | 64 | 1 | 0.001 | At each layer |
| 32 | 64 | 128 | 64 | 1 | 0.005 | At each layer |
Table: The Sensitivity of Learning Rate
{#tbl:rates-setting}
